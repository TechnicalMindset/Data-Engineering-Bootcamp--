{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY7zI+LhE5gQDFEC6sLEV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechnicalMindset/Data-Engineering-Bootcamp--/blob/main/Web_Scraping_%26_Data_Extraction_with_Python_LAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# 🌟 Web Scraping & Data Extraction with Python\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 **Introduktion – Vad är ETL?**\n",
        "\n",
        "**ETL** står för:\n",
        "\n",
        "- **Extract** – Hämta data (t.ex. från webbsidor eller API:er) 🌐\n",
        "- **Transform** – Bearbeta datan till ett användbart format 🔧\n",
        "- **Load** – Spara datan i en databas eller fil 📁\n",
        "\n",
        "Det används ofta i datavetenskap och affärsanalys för att samla in, rensa och lagra data från olika källor.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 **Labbmål (Objectives)**\n",
        "\n",
        "✅ Extrahera data från webbsidor och API:er  \n",
        "✅ Läsa in data från CSV, JSON och XML  \n",
        "✅ Transformera datan till enhetligt format  \n",
        "✅ Spara data i CSV och ladda till en SQLite-databas\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 **Del 1: Installera nödvändiga bibliotek**\n",
        "\n",
        "```python\n",
        "!pip install requests beautifulsoup4 pandas lxml\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🕸️ **Del 2: Web scraping – Extrahera topp 50 filmer**\n",
        "\n",
        "### 🔽 **Steg 1: Importera bibliotek**\n",
        "\n",
        "```python\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔽 **Steg 2: Hämta webbsidan**\n",
        "\n",
        "```python\n",
        "url = \"https://web.archive.org/web/20230902185655/https://en.everybodywiki.com/100_Most_Highly-Ranked_Films\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Kontrollera att sidan laddades korrekt\n",
        "print(\"Statuskod:\", response.status_code)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔽 **Steg 3: Analysera och extrahera HTML**\n",
        "\n",
        "```python\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Hitta tabellen med filmer\n",
        "table = soup.find(\"table\")\n",
        "\n",
        "# Hämta alla rader i tabellen (förutom rubrikraden)\n",
        "rows = table.find_all(\"tr\")[1:]\n",
        "\n",
        "# Skapa en lista för att lagra resultaten\n",
        "movies_data = []\n",
        "\n",
        "for row in rows[:50]:  # Begränsa till topp 50\n",
        "    cols = row.find_all(\"td\")\n",
        "    if len(cols) >= 3:\n",
        "        average_rank = cols[0].text.strip()\n",
        "        film = cols[1].text.strip()\n",
        "        year = cols[2].text.strip()\n",
        "        movies_data.append([average_rank, film, year])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔽 **Steg 4: Spara datan i en DataFrame**\n",
        "\n",
        "```python\n",
        "df_movies = pd.DataFrame(movies_data, columns=[\"Average Rank\", \"Film\", \"Year\"])\n",
        "df_movies.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 💾 **Del 3: Spara till CSV och SQLite**\n",
        "\n",
        "### 🔽 **Spara till CSV-fil**\n",
        "\n",
        "```python\n",
        "df_movies.to_csv(\"top_50_films.csv\", index=False)\n",
        "print(\"✅ Sparad till top_50_films.csv\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔽 **Spara till SQLite-databas**\n",
        "\n",
        "```python\n",
        "# Skapa anslutning\n",
        "conn = sqlite3.connect(\"Movies.db\")\n",
        "\n",
        "# Spara till tabell \"Top_50\"\n",
        "df_movies.to_sql(\"Top_50\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "# Bekräfta lagring\n",
        "print(\"✅ Datan har sparats i Movies.db i tabellen 'Top_50'\")\n",
        "\n",
        "# Stäng anslutningen\n",
        "conn.close()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📄 **Del 4: Läsa in data från CSV, JSON, XML**\n",
        "\n",
        "### 🔽 **CSV**\n",
        "\n",
        "```python\n",
        "df_csv = pd.read_csv(\"top_50_films.csv\")\n",
        "df_csv.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔽 **JSON (konvertera till JSON först)**\n",
        "\n",
        "```python\n",
        "df_csv.to_json(\"movies.json\", orient=\"records\", lines=True)\n",
        "\n",
        "# Läs tillbaka från JSON\n",
        "df_json = pd.read_json(\"movies.json\", lines=True)\n",
        "df_json.head()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔽 **XML (konvertera till XML)**\n",
        "\n",
        "```python\n",
        "def df_to_xml(df, root_tag=\"Movies\", row_tag=\"Movie\"):\n",
        "    from xml.etree.ElementTree import Element, tostring, SubElement\n",
        "    root = Element(root_tag)\n",
        "    for _, row in df.iterrows():\n",
        "        movie = SubElement(root, row_tag)\n",
        "        for col in df.columns:\n",
        "            child = SubElement(movie, col.replace(\" \", \"\"))\n",
        "            child.text = str(row[col])\n",
        "    return tostring(root, encoding=\"unicode\")\n",
        "\n",
        "# Skapa och spara XML\n",
        "xml_data = df_to_xml(df_csv)\n",
        "with open(\"movies.xml\", \"w\") as f:\n",
        "    f.write(xml_data)\n",
        "\n",
        "print(\"✅ XML-fil skapad: movies.xml\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 **Slutsats: Vad har vi lärt oss?**\n",
        "\n",
        "✅ Extrahera data från HTML med BeautifulSoup  \n",
        "✅ Spara och läsa data i CSV, JSON och XML  \n",
        "✅ Transformera data till tabellformat  \n",
        "✅ Ladda data till SQLite-databas\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VISsnXn2hayo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c59ziNkzhYb5"
      },
      "outputs": [],
      "source": []
    }
  ]
}